{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd4eebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.nnx as nnx\n",
    "import optax\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf025ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DqnModel(nnx.Module):\n",
    "  def __init__(self, inSize: int, outSize: int, rngs: nnx.Rngs):\n",
    "    intermediateSize = 512\n",
    "    key = rngs.params()\n",
    "    self.linear1 = nnx.Linear(inSize, intermediateSize, rngs=rngs)\n",
    "    self.linear2 = nnx.Linear(intermediateSize, outSize, rngs=rngs)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    x = self.linear1(x)\n",
    "    x = jax.nn.relu(x)\n",
    "    x = self.linear2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a2135a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DqnModel(75, 36, nnx.Rngs(0))\n",
    "targetModel = DqnModel(75, 36, nnx.Rngs(1))\n",
    "adam = optax.adam(1e-5)\n",
    "optimizer = nnx.Optimizer(model, adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75713890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute weighted loss and TD error for a SINGLE transition\n",
    "def computeWeightedLossAndTdErrorSingle(model, targetModel, transition, weight):\n",
    "  observation, selectedAction, isTerminal, reward, nextObservation = transition\n",
    "\n",
    "  gamma = 0.99\n",
    "\n",
    "  # --- DDQN Target Calculation ---\n",
    "  # Define the calculation for the non-terminal case using DDQN logic\n",
    "  def ddqnTargetCalculation(_):\n",
    "    # 1. Select the best action for nextObservation using the *main* model's Q-values.\n",
    "    #    We don't need gradients for this action selection step itself.\n",
    "    qValuesNextStateMain = model(nextObservation)\n",
    "    bestNextAction = jnp.argmax(qValuesNextStateMain)\n",
    "\n",
    "    # 2. Evaluate the Q-value of that *selected* action using the *target* model.\n",
    "    qValuesNextStateTarget = targetModel(nextObservation)\n",
    "    # Index the target Q-values with the action chosen by the main model\n",
    "    targetQValueForBestAction = qValuesNextStateTarget[bestNextAction]\n",
    "\n",
    "    # 3. Calculate the final target value (reward + discounted future value).\n",
    "    #    Stop gradients from flowing back through the target network calculation.\n",
    "    return reward + gamma * jax.lax.stop_gradient(targetQValueForBestAction)\n",
    "\n",
    "  targetValue = jax.lax.cond(\n",
    "      isTerminal,\n",
    "      lambda _: reward,\n",
    "      ddqnTargetCalculation,\n",
    "      None\n",
    "  )\n",
    "\n",
    "  # Prediction from main model\n",
    "  values = model(observation)\n",
    "  pred = values[selectedAction]\n",
    "\n",
    "  # Calculate Huber loss (unweighted)\n",
    "  unweightedLoss = optax.losses.huber_loss(pred, targetValue)\n",
    "\n",
    "  # Calculate TD Error (unweighted)\n",
    "  tdError = targetValue - pred\n",
    "\n",
    "  # Apply Importance Sampling weight to the loss\n",
    "  weightedLoss = weight * unweightedLoss\n",
    "\n",
    "  return weightedLoss, (tdError, jnp.min(values), jnp.mean(values), jnp.max(values))\n",
    "\n",
    "def computeWeightedLossAndTdErrorBatch(model, targetModel, transitions, weights):\n",
    "  batched = jax.vmap(computeWeightedLossAndTdErrorSingle, in_axes=( None, None, (0, 0, 0, 0, 0), 0 ), out_axes=(0, 0))\n",
    "  weightedLosses, (tdErrors, minValues, meanValues, maxValues) = batched(model, targetModel, transitions, weights)\n",
    "  return jnp.mean(weightedLosses), (jnp.mean(tdErrors), jnp.mean(minValues), jnp.mean(meanValues), jnp.mean(maxValues))\n",
    "\n",
    "@nnx.jit\n",
    "def train(model, optimizerState, targetModel, observation, selectedAction, isTerminal, reward, nextObservation, weight):\n",
    "  # print(f'observation: {type(observation)}: {observation}')\n",
    "  # print(f'selectedAction: {type(selectedAction)}: {selectedAction}')\n",
    "  # print(f'isTerminal: {type(isTerminal)}: {isTerminal}')\n",
    "  # print(f'reward: {type(reward)}: {reward}')\n",
    "  # print(f'nextObservation: {type(nextObservation)}: {nextObservation}')\n",
    "  # print(f'weight: {type(weight)}: {weight}')\n",
    "\n",
    "  (loss, auxOutput), gradients = nnx.value_and_grad(computeWeightedLossAndTdErrorBatch, has_aux=True)(model, targetModel, (observation, selectedAction, isTerminal, reward, nextObservation), weight)\n",
    "  optimizerState.update(gradients)\n",
    "  return auxOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1eba069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = np.array([[1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.]])\n",
    "selectedAction = [0]\n",
    "isTerminal = [False]\n",
    "reward = [0.0]\n",
    "nextObservation = np.array([[1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.]])\n",
    "weight = [1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3b68e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "npSelectedAction: <class 'numpy.ndarray'>: [0]\n",
      "npIsTerminal: <class 'numpy.ndarray'>: [False]\n",
      "npReward: <class 'numpy.ndarray'>: [0.]\n",
      "npWeight: <class 'numpy.ndarray'>: [1.]\n"
     ]
    }
   ],
   "source": [
    "npSelectedAction = np.array(selectedAction)\n",
    "npIsTerminal = np.array(isTerminal)\n",
    "npReward = np.array(reward)\n",
    "npWeight = np.array(weight)\n",
    "print(f'npSelectedAction: {type(npSelectedAction)}: {npSelectedAction}')\n",
    "print(f'npIsTerminal: {type(npIsTerminal)}: {npIsTerminal}')\n",
    "print(f'npReward: {type(npReward)}: {npReward}')\n",
    "print(f'npWeight: {type(npWeight)}: {npWeight}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4c6f4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(-0.14731956, dtype=float32),\n",
       " Array(-0.9599196, dtype=float32),\n",
       " Array(-0.06241585, dtype=float32),\n",
       " Array(1.0386813, dtype=float32))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model, optimizer, targetModel, observation, npSelectedAction, npIsTerminal, npReward, nextObservation, npWeight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
